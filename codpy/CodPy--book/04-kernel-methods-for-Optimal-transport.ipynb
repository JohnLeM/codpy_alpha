{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a1471bc1",
   "metadata": {},
   "source": [
    "---\n",
    "output:\n",
    "  pdf_document:\n",
    "    keep_tex: yes\n",
    "    includes:\n",
    "      in_header: preamble.tex\n",
    "  html_document: default\n",
    "  word_document: default\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb673f",
   "metadata": {
    "code": "#R_CODE#xfun::read_utf8('preamble.R')",
    "lines_to_next_cell": 2,
    "tags": [
     "remove_cell",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387d786",
   "metadata": {
    "tags": [
     "remove_input",
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "from preamble import *\n",
    "from stat_tools import *\n",
    "from scikit_tools import *\n",
    "import scipy\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bff287",
   "metadata": {
    "tags": [
     "remove_input",
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "from Bachelier import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143efa8",
   "metadata": {},
   "source": [
    "# Kernel methods for optimal transportation\n",
    "\n",
    "## Discrete ordering algorithms\n",
    "\n",
    "We describe in this section discrete ordering algorithms based on the kernel-based distance.\n",
    "\n",
    "### Linear Sum Assignment Problems (LSAP)\n",
    "\n",
    "In this section, we describe a ordering algorithm, which we discovered while working in the development of algorithms based on the theory of Reproducing Kernel Hilbert Space (RKHS). This algorithm is motivated by the ``linear assignment value'' problem, and is used in a number of our Academic and industrial applications; \n",
    "\n",
    "The Linear Sum Assignment problem (LSAP) is a fundamental problem of combinatorial optimization. It is an old and well-documented problem \\footnote{see the wikipedia page \\url{https://en.wikipedia.org/wiki/Assignment_problem}}, which leads to a large number of important industrial applications. It has been solved in the early 30's by H.W. Kuhn and is often called the Hungarian method in order to highlight that it derives from two older\\footnote{These algorithms might be credited to Jacobi posthumous papers \\cite{CGJacobi:1890}.} results by two Hungarians mathematicians, Koenig (Math Ann 77:453 465, 1916) and Egervry (Mat Fiz Lapok 38:1628, 1931). In particular, the approach adopted in the present text relies on the LSAP. As our library embeds a different algorithm than the Hungarian one for performance purposes, we describe and exhibit our interface on this problem in the next section.\n",
    "\n",
    "Our basic idea here is to use a ordering algorithm and order any two sets of points $x \\in \\RR^{N_x \\times D}$, $y \\in \\RR^{N_y \\times D}$ with respect to the *discrepancy distance matrix* $d_k(x,y) \\in \\RR^{N_x \\times N_y}$ associated with a given kernel, introduced in \\@ref(eq:norm). Without loss of generality, suppose that $N_x > N_y$. The above distance allows us to compute a permutation $\\sigma$ with length $N_y$, with the help of the LSAP algorithm. Then, we reorder and output the distribution $x$ with this permutation. Our motivation comes from optimal transportation; see for instance \\cite{Villani:2009} for a review of optimal transport. Indeed, once computed, the map\n",
    "$$\n",
    "  x^n \\mapsto y^{\\sigma_n}\n",
    "$$\n",
    "defines an optimal map, with respect to the kernel-induced *discrepancy distance* $d_k(x,y)$, described in section \\@ref(Discrepancy-error), transporting the measure $\\mu_x := \\sum_n \\delta_{x^n}$ into the measure $\\mu_y := \\sum_n \\delta_{y^{\\sigma_n}}$. This equivalence between the LSAP and the Monge-Kantorovich problem, used since some time, has only been recently rigorously proven in \\cite{Brezis:2018}.\n",
    "\n",
    "#### Description of the problem\n",
    "\n",
    "Linear sum assignment problems can be well described using graph theory : it consists of finding, in a weighted bipartite graph, a matching of a given size, in which the sum of weights of the edges is a minimum.\n",
    "\n",
    "To make this definition clear, let us introduce some notations : consider any real-valued matrix $M \\in \\RR^{N_x \\times N_y}$, called a *cost* matrix. A linear assignment problem consist in finding a permutation $\\sigma : [1 \\ldots \\min(N_x,N_y)] \\mapsto [1 \\ldots \\min(N_x,N_y)]$ such that\n",
    "$$\n",
    "  \\sigma = \\arg \\inf_{\\sigma \\in \\Sigma} \\sum_{n \\le \\min(N_x,N_y)} M(n,\\sigma(n)),\n",
    "$$\n",
    "where $\\Sigma$ holds here for the set of all permutations.\n",
    "Another equivalent formulation is the following one\n",
    "$$\n",
    "  \\sigma = \\arg \\inf_{\\sigma \\in \\Sigma} \\sigma \\cdot M\n",
    "$$\n",
    "where $A\\cdot B$ holds here for the Frobenius scalar product and $\\Sigma$ is the set of permutations, using a matrix representation : $\\sigma \\in \\RR^{N_x \\times N_y}$ $\\sum_n \\sigma(n,m) = \\sum_m \\sigma(n,m) = 1, \\sigma(n,m) \\in \\{0,1\\}$.\n",
    "\n",
    "Let us give a quick illustration for better understanding to this problem. We fill out a matrix with random values in table \\@ref(tab:512), and output also its cost, that is $Tr(M)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(M):\n",
    "  cost = 0.\n",
    "  for n in range(len(M)): cost += M[n,n]\n",
    "  return cost\n",
    "  \n",
    "N = 4\n",
    "M = np.random.rand(N,N)\n",
    "M_df = pd.DataFrame(M)\n",
    "print(\"total cost:\",cost(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c90630",
   "metadata": {
    "label": 512
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$M_df, caption = \"a 4x4 random matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c077c1",
   "metadata": {},
   "source": [
    "Then we compute the permutation $\\sigma$. The python interface to this function is simply $\\sigma = \\text{lsap}(M)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a92d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = alg.lsap(M)\n",
    "print(\"permutation: \",permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40feb02",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We reorder $\\tilde{M} = \\sigma M$, and we ouput the new cost after ordering, that is $Tr(\\tilde{M})$. we check in the following that the lsap algorithm decreased the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = M[permutation]\n",
    "print(\"total cost:\",cost(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438426e",
   "metadata": {},
   "source": [
    "#### Description of the algorithm\n",
    "\n",
    "Our ordering algorithm is quite straightfoward. Consider any two set of points $x \\in \\RR^{N_x \\times D}$, $y \\in \\RR^{N_y \\times D}$, a kernel $k(x,y)$, and the \\textit{discrepancy} distance $d_k(x,y) \\in \\RR^{N_x \\times N_y}$ introduced in \\@ref{distance-matrices}. Suppose wlog $N_y < N_x$. We compute first the  permutation\n",
    "$$\n",
    "  \\sigma = \\text{lsap}(d_k(x,y)), \\quad \\sigma \\in \\RR^{N_y}\n",
    "$$\n",
    "\n",
    "Then output the reordered set\n",
    "$$\n",
    "  x^\\sigma := (x^{\\sigma_1},\\ldots,x^{\\sigma_{N_y}}), \\quad y,\\quad \\text{ if }N_x \\ge N_y\n",
    "$$\n",
    "\n",
    "#### Description of the python function\n",
    "\n",
    "The ordering algorithm takes two distributions in input, and output a permutation of one of its input data ($x$ or $y$), as well as the permutation $\\sigma$:\n",
    "\n",
    "$$\n",
    "  x^\\sigma,y^\\sigma,\\sigma = alg.reordering(x,y,set\\_codpy\\_kernel, rescale,distance = None)\n",
    "$$\n",
    "\n",
    "This algorithm takes in input the following:\n",
    "\n",
    "* Two distributions of points having shapes\n",
    "$$\n",
    "  x := (x^1,\\ldots,x^{N_x}) \\in \\mathbb{R}^{N_x \\times D}, \\quad y:=(y^1,\\ldots,y^{N_y}) \\in \\mathbb{R}^{N_y \\times D}\n",
    "$$\n",
    "* A positive kernel $k(x,y)$, defined through the input variable set\\_codpy\\_kernel. This defines the cost matrix as being $M = d_k(x,y)$, where the distance matrix is defined in \\@ref(distance-matrices).\n",
    "\n",
    "* Alternatively an optional parameter $distance$ taking values among\n",
    "  * \"norm1\", in which case the sorting is done accordingly to the Manhattan distance $d(x,y) = |x-y|_1$\n",
    "  * \"norm2\", in which case the sorting is done accordingly to the Euclidean distance $d(x,y) = |x-y|_2$\n",
    "  * \"normifty\", in which case the sorting is done accordingly to the sup-distance $d(x,y) = |x-y|_\\infty$\n",
    "\n",
    "This function outputs :\n",
    "\n",
    "* Two distributions $x^\\sigma,y^ \\sigma$ having length $N_y$. If $N_x > N_y$, then $y^\\sigma=y$.\n",
    "The case $N_y>N_x$ is symetrical, letting the original distribution $x$ unchanged.\n",
    "* A permutation $\\sigma$, represented as a vector $i \\mapsto \\sigma_i$, $0 \\le i \\le \\min(N_x,N_y)$.\n",
    "\n",
    "#### Illustration for the square matrix case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75b3d0",
   "metadata": {},
   "source": [
    "##### A quantitative illustration\n",
    "\n",
    "We show first the results given by our ordering algorithm on a simple example. We generate two distributions of 4 points in $\\mathbb{R}^{5}$. The first is generated by multivariate Gaussian distribution centered in $(5,..,5)$, the second one by a uniform distribution supported into the unit cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240666aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "D = 5\n",
    "x0,y0 = np.random.normal(5., 1., (N,D)),np.random.rand(N,D)\n",
    "x_df,y_df = pd.DataFrame(x0),pd.DataFrame(y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19174f9",
   "metadata": {},
   "source": [
    "We output x in table \\@ref(tab:679) and y in table \\@ref(tab:680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fbac9",
   "metadata": {
    "label": 679
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$x_df, caption = \"a random gaussian distribution x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf15e7",
   "metadata": {
    "label": 680
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$y_df, caption =\"a random uniform distribution y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0d1d0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us first pick up a kernel $k$, here a Matern kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_codpy_kernel = kernel_setters.kernel_helper(kernel_setters.set_matern_tensor_kernel,2,1e-8,map_setters.set_mean_distance_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a214355",
   "metadata": {},
   "source": [
    "Then we compute the distance matrix, and output the transportation cost $\\sum_{n=0}^N d_k(n,n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e8fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dnm = op.Dnm(x0,y0,set_codpy_kernel = set_codpy_kernel,rescale=True)\n",
    "Dnm_df = pd.DataFrame(Dnm)\n",
    "print(\"cost:\", cost(Dnm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b58bfd",
   "metadata": {},
   "source": [
    "We output the distance matrix in table \\@ref(tab:681)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0fc86",
   "metadata": {
    "caption": "Distance matrix (samples)",
    "label": 681
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$Dnm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e9b99",
   "metadata": {},
   "source": [
    "We then invoke the ordering algorithm and output the cost after ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,permutation = alg.reordering(x0,y0,set_codpy_kernel = None, rescale = False)\n",
    "Dnm = op.Dnm(x,y,set_codpy_kernel = None, rescale = False)\n",
    "Dnm_df = pd.DataFrame(Dnm)\n",
    "print(\"cost:\", cost(Dnm))\n",
    "permutation = np.asarray(permutation)[0:4].reshape(1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bac6d",
   "metadata": {},
   "source": [
    "Finally, we output the distance matrix again after ordering in table \\@ref(tab:682), as well as the permutation $\\sigma$ in table \\@ref(tab:683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a1d8e",
   "metadata": {
    "label": 682,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$Dnm_df,caption = \"Matrix after ordering (samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ca455",
   "metadata": {
    "label": 683
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$permutation,caption = \"permutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a58344",
   "metadata": {},
   "source": [
    "One can check that the sum of the diagonal elements has decreased.\n",
    "\n",
    "##### A qualitative illustration\n",
    "\n",
    "This algorithm can be best illustrated in the two-dimensional case.\n",
    "\n",
    "We first consider the Euclidean distance function $d(x,y) = |x-y|$, in which case this algorithm  corresponds to a classical rearrangement, i.e. the one corresponding to the Wasserstein distance. To illustrate this behavior, let us generate a bi-modal type distribution $x \\in \\RR^{N \\times D}$ and a random uniform one $y \\in [0,1]^{N \\times D}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11604923",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "N=16\n",
    "D = 2\n",
    "left = np.random.normal(-3., 1., (int(N/2),D))\n",
    "right = np.random.normal(3., 1., (int(N/2),D))\n",
    "x0 = np.concatenate( (left,right) )\n",
    "y0 = np.random.rand(len(x0),D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e62018",
   "metadata": {},
   "source": [
    "For a convex distance, this algorithm is characterized by a ordering where characteristic lines do not cross each others, as plot in the picture \\@ref(fig:185), plotting both edges $x^i \\mapsto y^i$, before and after the ordering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba1653",
   "metadata": {
    "fig.cap": "LSAP with different input sizes",
    "label": 185
   },
   "outputs": [],
   "source": [
    "x,y,permutation = alg.reordering(x=x0,y=y0, set_codpy_kernel = None, distance = \"norm2\")\n",
    "reordering_plot(x0,y0,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11cf6a",
   "metadata": {},
   "source": [
    "Note however that kernels based distance might lead to different permutations. This is due to the fact that kernels defines distance that might not be euclidean. Indeed, kernel distance might not respect the triangular inequality. For instance, the kernel selected above defines a distance equivalent to $d(x,y) = \\Pi_d |x_d-y_d|$, and leads to a ordering for which some characteristics should cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,permutation = alg.reordering(x=x0,y=y0, set_codpy_kernel = set_codpy_kernel, rescale = True)\n",
    "reordering_plot(x0,y0,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd36a865",
   "metadata": {},
   "source": [
    "### LSAP extensions\n",
    "\n",
    "In this section we describe some extensions of the LSAP algorithms that we use in our library.\n",
    "\n",
    "#### Different input sizes\n",
    "\n",
    "A first quite straightforward extension of LSAP problem can be found for inputs set of different sizes, wlog $N_y\\le N_x$. The figure \\@ref(fig:184) illustrates the behavior of our LSAP algorithm in this setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb5f57",
   "metadata": {
    "fig.cap": "LSAP with different input sizes",
    "label": 184,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "N=32\n",
    "M=16\n",
    "D = 2\n",
    "left = np.random.normal(-3., 1., (int(N/2),D))\n",
    "right = np.random.normal(3., 1., (int(N/2),D))\n",
    "x0 = np.concatenate( (left,right) )\n",
    "y0 = np.random.rand(M,D)\n",
    "x,y,permutation = alg.reordering(x=x0,y=y0, set_codpy_kernel = None, distance = \"norm2\")\n",
    "reordering_plot(x0,y0,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d394afd3",
   "metadata": {},
   "source": [
    "#### General cost functions and motivations\n",
    "\n",
    "Consider any  real-valued matrix $M \\in \\RR^{N \\times N}$. In situations of interests, we consider cost functional $c(M)$ that generalizes the classical cost functional for LSAP problem $c(M) = \\sum_{n} M(n,n)$. Our algorithm generalizes to these cases, finding a permutation $\\sigma : [1 \\ldots N] \\mapsto [1 \\ldots N]$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b840df5",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\bar{\\sigma} = \\arg \\inf_{\\sigma \\in \\Sigma} c( M^\\sigma),\\quad M^\\sigma = m(n,\\sigma(n)) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cf310",
   "metadata": {},
   "source": [
    "An example of such a LSAP problem extension arised with kernel methods in section \\@ref(sharp-discrepancy-sequences). It corresponds to compute the minimum of the discrepancy functional \\@ref{eq:dk}, for the particular choice where $x^\\sigma \\subset x$ is a subset of $x$ having length $N_y<N_x$. We used the notations $x^\\sigma=(x^{\\sigma_1},\\ldots,x^{\\sigma_{N_y}})$, with $\\sigma : [1\\ldots N_y] \\mapsto [1\\ldots N_x]$. In this context, the matrix is defined as $M(n,m) = k\\big(x^n,x^m\\big)$, and the cost function is\n",
    "\n",
    "$$\n",
    "d_k\\big(x,x^\\sigma\\big)^2 = c(M) = \\frac{1}{N_x^2}\\sum_{n=1,m=1}^{N_x,N_x} M(n,m) + \\frac{1}{N_y^2}\\sum_{n=1,m=1}^{N_y,N_y} M(\\sigma(n),\\sigma(m)) - \\frac{2}{N_x N_y}\\sum_{n=1,m=1}^{N_x,N_y} k\\big(n,\\sigma(m)\\big).\n",
    "$$\n",
    "\n",
    "So that our target minimization problem can be described as finding a permutation $\\bar{\\sigma}$ such that\n",
    "\n",
    "$$\n",
    " \\overline{\\sigma} = \\arg \\inf_{\\sigma : [1\\ldots N_y] \\mapsto [1\\ldots N_x]} c(M^\\sigma\\big),\\quad M^\\sigma(n,m) = k(x^n,x^{\\sigma(m)})\n",
    "$$\n",
    "\n",
    "## Conditional expectation algorithm\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this section, we propose a general interface to a python function computing conditional expectations problems in arbitrary dimensions, that we named Pi. We also propose a kernel-based implementation of these problems, which algorithm is described in \\cite{LeFloch-Mercier:2017} - \\cite{LeFloch-Mercier:2020b}. \n",
    "\n",
    "Kernel methods to compute conditional expectations started to be considered a decade ago, see for instance \\cite{Mercier:2014}. Indeed, these algorithms are centrals, particularly for finance applications, as they are the heart of pricing technologies. They also have numerous other applications. \n",
    "\n",
    "Benchmarking such algorithms is a difficult task, as the literature did not provide competitor algorithms to compute conditional expectations to kernel-based methods, for arbitrary dimensions, to our knowledge. Indeed, these algorithms are tightly concerned with the so called \\textit{curse of dimensionality}, as we are dealing with arbitrary dimensions algorithms. \n",
    "\n",
    "However, there is a recent, but impressively fast-growing, literature, devoted to the study of Artificial Intelligence methods (AI), particularly for Finance applications, see \\cite{GPW} and ref. therein for instance. In particular, a Neural Networks (NN) approach has been proposed to compute conditional expectation in \\cite{NS} that we can use as benchmark. Hence a first benchmark is conducted in section \\ref{the-bachelier-problem}.\n",
    "\n",
    "### The Pi function\n",
    "\n",
    "Consider any martingale process $t \\mapsto X(t)$, and any positive definite kernel $k$, we define the operator $\\Pi$ - using python notations -\n",
    "\n",
    "\\begin{equation}\\label{Pi}\n",
    "  f_{z | x} = \\Pi(x,z,f(z)=[])\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "- $x \\in \\RR^{ N_x \\times D}$ is any set of points generated by a i.i.d sample of $X(t^1)$ where $t^1$ is any time.\n",
    "\n",
    "- $z \\in \\RR^{ N_z \\times D}$ is any set of points, generated by a i.i.d sample of $X(t^2)$ at any time $t^2>t^1$.\n",
    "\n",
    "- $f(z) \\in \\RR^{ N_z \\times D_f}$ is any, optional, function, representing payoff values. \n",
    "\n",
    "The output is\n",
    "\n",
    "- if $f(z)$ is let empty, the output $f_{z | x} \\in \\RR^{ N_z \\times N_x}$ is a matrix, representing a convergent approximation of the stochastic matrix $\\EE^X(z | x)$.\n",
    "\n",
    "- if $f(z) \\in \\RR^{ N_z \\times D_f}$ is not empty, $f_{z | x} \\in \\RR^{ N_z \\times D_f}$ is a matrix, representing the conditional expectation $f(z |x) := \\EE^X( f(z) | x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b995fa4",
   "metadata": {},
   "source": [
    "## Polar factorization algorithms\n",
    "\n",
    "Consider any mapping $S : \\RR^D \\mapsto \\RR^D$, and a distance function, that is positive, scalar valued, $\\mathcal{C}^1$ function $d(\\cdot,\\cdot)$.\n",
    "\n",
    "The polar factorization algorithm amounts to find a **scalar, convex** function $f$, and a volume preserving map $T:\\RR^D \\mapsto \\RR^D$, satisfying\n",
    "\n",
    "$$\n",
    "  S = \\Big(\\nabla f\\Big)\\circ T(y),\\quad f \\text{ convex}, \\quad T_\\#m = m (\\#eq:PF)\n",
    "$$\n",
    "A volume preserving map is a mapping satisfying $\\int \\varphi dx = \\int \\varphi\\circ T dx$ for any continuous function $\\varphi$, $dx$ being the Lebesgue measure. Existence and unicity of this decomposition is discussed in Brenier seminal's paper \\cite{BY}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ceb81",
   "metadata": {},
   "source": [
    "### Discrete Polar factorization and linear sum assignment problem\n",
    "\n",
    "Let us start from any distributions $x \\in \\RR^{N_x \\times D}$, $z \\in \\RR^{N_y \\times D}$, and consider a distance function, that is positive, scalar valued, $\\mathcal{C}^1$ function $d(\\cdot,\\cdot)$, and we naturally extend this function to a matrix valued one $d(x,z) \\in \\RR^{N_x \\times N_z}$.\n",
    "Let us first make some reminder about optimal transportation type problems.\n",
    "\n",
    "### The Monge-Kantorovitch problem\n",
    "\n",
    "Polar factorization are linked to the Monge-Kantorovitch problem, that is an optimal transportation one. Consider any two measures $\\mu_x$, $\\mu_z$, and consider a distance function, that is positive, symmetrical, convex, scalar valued, $\\mathcal{C}^1$ function $d(\\cdot,\\cdot)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b844e19",
   "metadata": {},
   "source": [
    "The following discrete problem is called the **Monge** problem\n",
    "$$\n",
    "\\bar{\\gamma} = \\inf_{\\gamma \\in \\Gamma} d(x,z)\\cdot \\gamma  (\\#eq:MO)\n",
    "$$\n",
    "where \n",
    "\n",
    "* $A \\cdot B$ denotes the Frobenius scalar matrix product\n",
    "* $\\Gamma$ denotes the set of all bi-stochastic matrix $\\gamma \\in \\RR^{N \\times N}$, that is satisfying,\n",
    "$$\n",
    "\\sum_{n = 1\\ldots N} \\gamma_{m,n} =  \\sum_{n = 1\\ldots N} \\gamma_{n,m} = 1, \\quad \\gamma_{n,m}\\ge 0, \\quad \\text{for all } n,m = 1,\\ldots,N.\n",
    "$$\n",
    "\n",
    "This minimization problem owns a dual expression, called the **Kantorovitch** problem\n",
    "\n",
    "$$\n",
    " \\sup_{\\varphi, \\psi } \\sum_n^N \\varphi(x^n) - \\psi(z^n),\\quad \\varphi(x^n) - \\psi(z^n) \\le d(x^n,z^n)  (\\#eq:KA)\n",
    "$$\n",
    "where $\\varphi,\\psi$ are the unknown functions.\n",
    "\n",
    "Note that any permutation $\\sigma:[1\\ldots N]\\mapsto [1\\ldots N]$ is a stochastic matrix. In particular, the following discrete problem, that is a Linear assignment problem, described in section \\@ref(linear-sum-assignment-problems-lsap), consists in a first approach to \\@ref(eq:MO):\n",
    "\n",
    "$$\n",
    "  \\bar{\\gamma} = \\inf_{\\gamma \\in \\Sigma} d(x,z)\\cdot \\gamma  (\\#eq:MKII)\n",
    "$$\n",
    "Indeed, all problems \\@ref(eq:MO)-\\@ref(eq:MKII)-\\@ref(eq:KA) are equivalent, see \\cite{Brezis:2018}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d8f10",
   "metadata": {},
   "source": [
    "### Motivation: the sampler function\n",
    "\n",
    "In many applications we would like to fit the scattered data to a given model that best represents them. To be specific, consider any distributions of points $x \\in \\RR^{N \\times D}$, representing i.i.d. samples of a random variable $X$, $z \\in \\RR^{[0,1]^{N \\times D}}$, any i.i.d. of the uniform distribution into the unit cube, and suppose that we solved \\@ref(eq:PF) in the following, discrete, sense\n",
    "\n",
    "$$\n",
    "  x = \\Big(\\nabla f\\Big)(z),\\quad f \\text{ convex}, \\quad x \\in \\RR^{N \\times D}, z \\in [0,1]^{N \\times D}.  (\\#eq:fxz)\n",
    "$$\n",
    "\n",
    "Then the function \n",
    "$$\n",
    "  y \\mapsto \\Big(\\nabla f\\Big)(y), (\\#eq:fy)\n",
    "$$\n",
    "where $y \\in \\RR^{[0,1]^{N_y \\times D}}$ provides us with a natural candidate for others i.i.d. realization of the random variable $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fe91e",
   "metadata": {},
   "source": [
    "Hence this section illustrates the following python function\n",
    "$$ \n",
    "    y = sampler(x,M, seed) (\\#eq:sampler)\n",
    "$$\n",
    "that outputs $M$ values $y \\in \\mathbb{R}^{N\\times D}$ of a distribution sharing close statistical properties with the discrete distribution $x$, that we discuss in the next paragraph.\n",
    "\n",
    "#### Statistical tests\n",
    "\n",
    "We exhibit three statistical indicators to support our claims, measuring each some kind of distance between the two distributions $x$ and $y$. The two first tests are one-dimensional based tests. We check it on every axes. The third one is based on the discrepancy error.\n",
    "\n",
    "* Kolmogorov-Smirnov based tests. These are one-dimensional tests, based on the cumulative distribution function. The test is\n",
    "$$\n",
    "  \\|cdf_x  - cdf_z\\|_\\ell^\\infty(\\RR^N) \\ge \\frac{c_N}{\\sqrt{N}} \n",
    "$$\n",
    "where $c_N$ is a confidence level.\n",
    "* Hellinger distance tests. To measure the closedness of $pdf_x$ to $pdf_z$ which are the PDFs of $x$ and $y$ repectively. The Hellinger distance is defined as\n",
    "$$\n",
    "\\mathcal{H}(x,z) = \\frac{1}{\\sqrt{2}}||\\sqrt{pdf_x} - \\sqrt{pdf_z}||_2\n",
    "$$\n",
    "* Discrepancy errors, defined in section \\@ref(discrepancy-error).\n",
    "\n",
    "### One dimensional Examples\n",
    "\n",
    "#### Bimodal Gaussian distribution\n",
    "\n",
    "In this section we study a bi-modal gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bdb013",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "(N,D) = (1000,1)\n",
    "x = np.random.normal(-5., 1., (int(N/2),D))\n",
    "x = np.concatenate( (x,np.random.normal(+5., 1., (int(N/2),D))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef0a39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We output some values of $x$ in the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b4fd4",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$x), caption = 'one-dimensional bi-modal distribution', label = 'Bi1D', col.names = c('samples bimodal distribution'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82561d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us call the sampling function, filling up $y \\in \\mathbb{R}^{M\\times 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adb64e",
   "metadata": {
    "include": true
   },
   "outputs": [],
   "source": [
    "M = 500\n",
    "y = alg.sampler(x,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97af46",
   "metadata": {},
   "source": [
    "We output some values of $y$ in the following lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb692d",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$y), caption = 'sampled distribution', label = 'Bi1DGen', col.names = c('generated samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb1965",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To check our results, let us compare both cdf of $x$ and $y$ in the following figure\n",
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08543231",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "compare_plot(x,y,title = \"original distribution (blue) versus generated distribution (yellow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd7ff6",
   "metadata": {},
   "source": [
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e79414",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 2,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "plot(ecdf(py$x), verticals=TRUE, do.points=FALSE, main = \"original versus generated CDFs (Gaussian bi-modal)\", \n",
    "     col = 'black', lwd = 2, xlab = \"x, y\", ylab = \"F(x), F(y)\")\n",
    "plot(ecdf(py$y), verticals=TRUE, do.points=FALSE, add=TRUE, col='blue', lwd = 2, lty = 2)\n",
    "legend(1,0.4, legend = c(\"Original (x)\", \"Generated (y)\"), col = c(\"black\", 'blue'), lty =1:2, lwd = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cbae7",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "summary_ = summary((x,y))\n",
    "ks_ = ks_testD(x,y)\n",
    "#dd = compare_distances(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599efe0",
   "metadata": {},
   "source": [
    "To check numerically some first properties of the generated distribution, We output in the following table the skewness and kurtosis of both $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f41bbb",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$summary_), caption = 'distributions characteristics', label = 'Ga1Dsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239eedd",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$ks_, caption = 'KS Test', label = 'BiKS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d33e061",
   "metadata": {},
   "source": [
    "#### Bimodal t-distribution\n",
    "\n",
    "In this section we study a bi-modal t - distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130daa61",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "(N,D, df) = (1000,1,3)\n",
    "x = np.concatenate( (scipy.stats.t.rvs(df, -5., 1.,  (N,D)), \n",
    "scipy.stats.t.rvs(df, 5., 1., (N,D))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d21f4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We output some values of $x$ in the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6bab4",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$x), caption = 'one-dimensional bi-modal distribution', label = 'Bi1D', col.names = c('samples bimodal distribution'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7b2f2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us call the sampling function, filling up $y \\in \\mathbb{R}^{M\\times 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66adc1c1",
   "metadata": {
    "include": true
   },
   "outputs": [],
   "source": [
    "M = 500\n",
    "y = alg.sampler(x,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e88a2e",
   "metadata": {},
   "source": [
    "We output some values of $y$ in the following lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a13be",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$y), caption = 'sampled distribution', label = 'Bi1DGen', col.names = c('generated samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13599c6a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To check our results, let us compare both cdf of $x$ and $y$ in the following figure\n",
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851fb02",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "compare_plot(x,y,title = \"original distribution (blue) versus generated distribution (yellow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb5646",
   "metadata": {},
   "source": [
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5faefb",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 2,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "plot(ecdf(py$x), verticals=TRUE, do.points=FALSE, main = \"original versus generated CDFs (bi-modal t distribution)\", \n",
    "     col = 'black', lwd = 2, xlab = \"x, y\", ylab = \"F(x), F(y)\")\n",
    "plot(ecdf(py$y), verticals=TRUE, do.points=FALSE, add=TRUE, col='blue', lwd = 2, lty = 2)\n",
    "legend(1,0.4, legend = c(\"Original (x)\", \"Generated (y)\"), col = c(\"black\", 'blue'), lty =1:2, lwd = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb61ee",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "summary_ = summary((x,y))\n",
    "ks_ = ks_testD(x,y)\n",
    "#dd = compare_distances(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf7b0d",
   "metadata": {},
   "source": [
    "To check numerically some first properties of the generated distribution, We output in the following table the skewness and kurtosis of both $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2543ce",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$summary_), caption = 'distributions characteristics', label = 'Ga1Dsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0216e",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$ks_, caption = 'KS Test', label = 'BiKS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ec6ec",
   "metadata": {},
   "source": [
    "### N dimensional Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563bac8",
   "metadata": {
    "include": true
   },
   "outputs": [],
   "source": [
    "(N,D) = (1000,2)\n",
    "x = np.random.normal(size = (N, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7922833",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us call the sampling function, filling up $y \\in \\mathbb{R}^{M\\times 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d3c59",
   "metadata": {
    "include": true
   },
   "outputs": [],
   "source": [
    "M = 1000\n",
    "y = alg.sampler(x,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dbf11",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To check our results, let us compare both cdf of $x$ and $y$ in the following figure\n",
    "\n",
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f3a59",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "compare_plot(x,y,title = \"original distribution (blue) versus generated distribution (yellow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c32e8",
   "metadata": {},
   "source": [
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a33a23",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "plot(py$x, py$y, col = c(1,2), pch = 19, main = \"original distribution (black) versus generated distribution (red)\",\n",
    "     ylab = \"y\", xlab = \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a45465",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "summary_ = summary((x,y))\n",
    "ks_ = ks_testD(x,y)\n",
    "dd = compare_distances(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3dc05",
   "metadata": {},
   "source": [
    "To check numerically some first properties of the generated distribution, We output in the following table the skewness and kurtosis of both $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eff8b",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$summary_), caption = 'distributions characteristics', label = 'Ga2Dsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bb245",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$ks_, caption = 'KS Test', label = 'Ga2DKS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d183690",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$dd, caption = 'Hell. dist. / discrepancy err.', label = 'dicrepancy error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9c493",
   "metadata": {},
   "source": [
    "#### ND t - distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64217e",
   "metadata": {
    "include": true
   },
   "outputs": [],
   "source": [
    "(N,D, df) = (1000,2,3)\n",
    "x = np.random.standard_t(df,size = (N, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46ca97",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us call the sampling function, filling up $y \\in \\mathbb{R}^{M\\times 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af5b6b",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "M = 1000\n",
    "y = alg.sampler(x,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29213522",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To check our results, let us compare both cdf of $x$ and $y$ in the following figure\n",
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd874a",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "compare_plot(x,y,title = \"original distribution (blue) versus generated distribution (yellow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa6cd5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f4fe3",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "plot(py$x, py$y, col = c(1,2), pch = 19, main = \"original distribution (black) versus generated distribution (red)\",\n",
    "     ylab = \"y\", xlab = \"x\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49731ce1",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "summary_ = summary((x,y))\n",
    "ks_ = ks_testD(x,y)\n",
    "dd = compare_distances(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a381bc",
   "metadata": {},
   "source": [
    "To check numerically some first properties of the generated distribution, We output in the following table the skewness and kurtosis of both $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980af812",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$summary_), caption = 'distributions characteristics', label = 'Ga1Dsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a6f6a",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$ks_, caption = 'KS Test', label = 'St1DKS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2a7ad",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$dd, caption = 'Hell. dist. / discrepancy err.', label = 'dicrepancy error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52285c",
   "metadata": {},
   "source": [
    "#### Nd-Bimodal Gaussian distribution\n",
    "\n",
    "In this section we study a bi-modal gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56e9f1",
   "metadata": {
    "include": true
   },
   "outputs": [],
   "source": [
    "(N,D) = (1000,2)\n",
    "x = np.random.normal(-5., 1., (int(N/2),D))\n",
    "x = np.concatenate( (x,np.random.normal(+5., 1., (int(N/2),D))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc702c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us call the sampling function, filling up $y \\in \\mathbb{R}^{M\\times 1}$, and let us plot both original and generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e3709",
   "metadata": {
    "include": true
   },
   "outputs": [],
   "source": [
    "M = 1000\n",
    "y = alg.sampler(x,M)\n",
    "compare_plot(x,y,title = \"original distribution (blue) versus generated distribution (yellow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8264891",
   "metadata": {},
   "source": [
    "To check our results, let us compare both cdf of $x$ and $y$ in the following figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255afca6",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "plot( py$x,py$y, pch = 19, main = \"original distribution (black) versus generated distribution (red)\",  ylab = \"y\", xlab = \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d55ef",
   "metadata": {
    "include": true,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "summary_ = summary((x,y))\n",
    "ks_ = ks_testD(x,y)\n",
    "dd = compare_distances(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a6c7f",
   "metadata": {},
   "source": [
    "To check numerically some first properties of the generated distribution, We output in the following table the skewness and kurtosis of both $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c35fa",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(head(py$summary_), caption = 'distributions characteristics', label = 'Ga1Dsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0d033",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$ks_, caption = 'KS Test', label = 'St1DKS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4418a",
   "metadata": {
    "include": true,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::kable(py$dd, caption = 'Hell. dist. / discrepancy err.', label = 'dicrepancy error')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "label,caption,tags,include,fig.cap,code,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
