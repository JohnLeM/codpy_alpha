{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5e0f5705",
   "metadata": {},
   "source": [
    "---\n",
    "output:\n",
    "  html_document: default\n",
    "  pdf_document:\n",
    "    keep_tex: true\n",
    "    includes:\n",
    "     in_header: \"preamble.tex\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220ff1f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207586a",
   "metadata": {
    "code": "#R_CODE#xfun::read_utf8('preamble.R')",
    "tags": [
     "remove_cell",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f2588",
   "metadata": {
    "tags": [
     "remove_input",
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "from preamble import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e43c76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Main objective  \n",
    "\n",
    "This monograph presents the algorithms that are implemented in the Python library CodPy, an acronym that stands for \"Curse of dimensionality in Python\". This library provides the user with a support vector machine (or SVM in short) which is application-oriented in the sense that it provides a package of techniques relevant for numerous applications. The proposed algorithms apply to systems of partial differential\n",
    "equations arising in mathematical finance and fluid dynamics, as well discrete models of machine learning and statistics. We rely on a numerical strategy based on the theory of reproducing kernel Hilbert spaces (RKHS) which the authors have developed over the past decade, originally for applications in mathematical finance.\n",
    "\n",
    "We proceed by presenting first, in several tutorial chapters, the basic notions of discretization, as we formulated them in CodPy, and we include elementary examples in order to illustrate the role of these main concepts. In a second part of this monograph, we apply our framework and include more sophisticated discretization techniques and numerical results, while covering applications in pattern recognition and mathematical finance. The proposed kernel engineering technique aims at formulating support vector machines in a way that makes it easy to adapt them to any particular problem. Our methodology encompasses the discretization of differential operators, which we naturally associate with any support vector machine. Indeed, discrete differential operators are building blocks in order to design discrete algorithms for partial differential equations, for instance those arising in fluid dynamics. Importantly, our metholology leads us to error bounds or quality tests, which are of crucial importance in many applications such as mathematical finance. \n",
    "\n",
    "We found it convenient to write this monograph by relying on a combination of Python code, R code, and Latex code in order to generate a Jupyter notebook. This has led us to a document in which all numerical tests can be repeated and modified by the user\\footnote{CodPy will be made available for all users in the near future.}. \n",
    "\n",
    "## Outline of this monograph \n",
    "\n",
    "### Aim of Chapter 2: a quick tour to machine learning\n",
    "\n",
    "* In section \\@ref(a-framework-to-machine-learning-methods), we overview the techniques of learning machines, and introduce the general notation that will be in order for the rest of this monograph. We point out the links between this description and thz standard terminology used in the machine learning community, and we review the numerous methods available in this field. We thus discuss the notions relevant for  \n",
    "\n",
    "  * the description of numerical methods for machine learning, \n",
    "\n",
    "  * the performance indicators that provide a measure to the relevance of any given learning machines, and \n",
    "\n",
    "  * we mention the class of libraries currently available.\n",
    "It is not our purpose to cover all of the techniques, but to focus on kernel-based methods for machine learning and many other applications, and contribute here with several new aspects of the subject. For instance, discrete projection operators (see section \\@ref(projection-operator)) and kernel-based clustering methods (see \\@ref(a-kernel-based-clustering-algorithm)) are novel algorithms. As we also advocate here, the notions of discrepancy error (see \\@ref(eq:err)) and kernel-based norms (see  Section \\@ref(eq:norm)) leads us to performance indicators that are particularly efficient in the applications.  \n",
    "\n",
    "* In Section \\@ref(performance-indicators-for-machine-learning), we list a set of criteria that can be used to evaluate the performance of an algorithm and do not depend on the specific method in use. We can thus, with the help of the previous step, automate the benchmark of existing methods. Indeed, due to the vast amount of existing machine learning approaches, we encourage the reader to systematically benchmark them:\n",
    "  * To a given learning problem, that is materialized as a list of input data. \n",
    "  * Pick a list of scenarios, a list of learning machines, and a list of performance indicators.\n",
    "  * Run the tests, output and compare performance indicators.\n",
    "* The two previous steps allowed us to implement a framework into which we can plug-in a quite large zoo of learning machines, to illustrate numerically with simple, one or two dimensional, examples, our purposes. The section \\@ref(a-simple-illustration-of-supervised-learning-method-benchmarking) (resp. \\@ref(a-simple-illustration-of-unsupervised-learning-methods-benchmarking)) contains examples and illustrations for supervised learning (resp. unsupervised) benchmarks.\n",
    "\n",
    "### Aim of Chapter 3\n",
    "\n",
    "### Aim of Chapter 4  \n",
    "\n",
    "### Aim of Chapter 5  \n",
    "\n",
    "### Aim of Chapter 6\n",
    "\n",
    "..................\n",
    " \n",
    "## Further references \n",
    "\n",
    "Since our primarily intention here is to provide a technical introduction to our Python library, we only include a brief bibliography here. While a large literature is  available which is devoted to support vector machines and  reproducing kernel Hilbert spaces (RKHS), it is not our purpose to review it here. \n",
    "We would like refer the reader to Berlinet and Thomas-Agnan  \\cite{BerlinetThomasAgnan:2004} and Fasshauer \\cite{Fasshauer:2006,Fasshauer:2007,Fasshauer} since they were the most influential in the development of the present code.\n",
    "The reader will find therein a background on the subject, together with many further references.  \n",
    "\n",
    "Our original contributions concerning the class of kernel-based mesh-free algorithms presented in this monograph can be found in the research papers by LeFloch and Mercier \\cite{LeFloch-Mercier:2015,LeFloch-Mercier:2017,LeFloch-Mercier:2020a,LeFloch-Mercier:2020b,LeFloch-Mercier:2021}.  Moreover, \\cite{LeFlochMercierMiryusupov:2021a}--\\cite{LeFlochMercierMiryusupov:2021f}  \n",
    "contain earlier versions of the material in this monograph. \n",
    "\n",
    "For additional results on kernel techniques, we refer to \\cite{NarcowichWardWendland:2005,Niederreiter:1992,Opfer:2006,Wendland:1997,Wendland:2005,Zwicknagl:2008. Mesh-less methods and kernel-based strategies have been found very useful in fluid dynamics and material dynamics \n",
    "\\cite{BabuskaBanerjeeOsborn:2003,BessaFosterBelytschkoLiu:2014,GuntherLiu:1998,HaghighatRaissibMoureGomezJuanes:2021,KorzeniowskiWeinberg:2021,LiLiu:2004,Liu:2016,Nakano:2017,OhDavisJeong:2012,SalehiDehghan:2013,SirignanoSpiliopoulos:2018,ZhouLi:2006}. \n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "code,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
